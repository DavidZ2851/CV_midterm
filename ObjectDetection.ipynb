{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "from Angle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObjectDection:\n",
    "    def __init__(self,device,isRealsense=False):\n",
    "        self.device = device\n",
    "        self.model = self.load_model()\n",
    "        self.isRealsense = isRealsense\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'custom',path = \"./best.pt\")\n",
    "        return model\n",
    "    \n",
    "    def camera_setup(self):\n",
    "        pipeline = rs.pipeline()\n",
    "        pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "        config = rs.config()\n",
    "        pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "        device = pipeline_profile.get_device()\n",
    "        device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "        # Start streaming\n",
    "        pipeline.start(config)\n",
    "        return pipeline\n",
    "     \n",
    "\n",
    "    def capture(self):\n",
    "        if self.isRealsense:\n",
    "            pipeline = self.camera_setup()\n",
    "\n",
    "        cap= cv2.VideoCapture(self.device)\n",
    "        ##cap.set(cv2.CAP_PROP_FRAME_HEIGHT,480)\n",
    "        ##cap.set(cv2.CAP_PROP_FRAME_WIDTH,640)\n",
    "        \n",
    "        while True:\n",
    "            if self.isRealsense:\n",
    "                ret,color_image,depth_image = self.get_frame(pipeline)\n",
    "            else:\n",
    "                ret,color_image = cap.read()\n",
    "                \n",
    "            if ret:\n",
    "                filtered_image  = self.get_blue_frame(color_image)\n",
    "\n",
    "                result_object = self.detect_object(filtered_image)\n",
    "                cords = self.write_result(result_object,color_image)\n",
    "                self.write_angle_offset(color_image,cords)\n",
    "                \"\"\"\n",
    "                if self.isRealsense== False:\n",
    "                    result_object_right = self.detect_object(color_image_2)\n",
    "                    cords_right = self.write_result(result_object_right,color_image_2)\n",
    "                    distances = self.get_depth(cords,cords_right)\n",
    "                    self.write_result_depth(color_image,cords,distances)\n",
    "                \"\"\"\n",
    "                cv2.imshow(\"frame\",color_image)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27 or key == ord(\"q\"):\n",
    "                    break\n",
    "        print(\"Finish\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "\n",
    "    def detect_object(self,color_frame):\n",
    "        #color_frame = [color_frame]\n",
    "        results = self.model(color_frame)\n",
    "        #labels,cord = results.xyxyn[0][:,-1], results.xyxyn[0][:,:-1]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def detect_depth(self,depth_image,point):\n",
    "            return depth_image[point[1], point[0]]\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.capture()\n",
    "\n",
    "    def get_frame(self,pipeline):\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        if not depth_frame or not color_frame:\n",
    "            return False, None, None\n",
    "        return True, depth_image, color_image\n",
    "\n",
    "\n",
    "    def get_depth(self,cords_left,cords_right):\n",
    "        x_center = 640/2 # The camera I used is 480p, 640 pixels * 480 pixels, the center is 640/2\n",
    "        y_center = 480/2\n",
    "        \n",
    "        cam_distance = 13 # distance between two cameras cm\n",
    "        fov = 90 # fov 90 degrees\n",
    "        length = min(len(cords_left),len(cords_right))\n",
    "        \n",
    "        f_pixel = x_center / np.tan(fov * 0.5 * np.pi/180) # find the pixel length cm/\n",
    "        distance = []\n",
    "        for i in range(length):\n",
    "            disparity = cords_left[i][0]-cords_right[i][1] # diparity in pixels\n",
    "            zDepth = f_pixel * cam_distance / disparity\n",
    "            distance.append(abs(zDepth))\n",
    "\n",
    "        return distance\n",
    "    \n",
    "    def get_blue_frame(self,frame):\n",
    "        \n",
    "        hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "        lower_blue = np.array([110,50,50])\n",
    "        upper_blue = np.array([130,255,255])\n",
    "        mask =  cv2.inRange(hsv,lower_blue,upper_blue)\n",
    "        contours,hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area >450:\n",
    "                x_min,y_min,width,height = cv2.boundingRect(contour)\n",
    "                ##print(x_min,y_min,width,height)\n",
    "                cv2.rectangle(mask,(x_min-30,y_min-30),(x_min+width+30,y_min+height+30),(2552,255,255),-1)\n",
    "        res = cv2.bitwise_and(frame,frame,mask = mask)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def write_angle_offset(self,frame,cords):\n",
    "        offset_calculator = Angle(70,42,640,360) ## Here the x_fov of intel D435i is 70 and y_fov is 42, the pixel of the video is 640* 360 \n",
    "        for cord in cords:\n",
    "            x_offset = offset_calculator.get_horizontal_offset(cord[0])\n",
    "            y_offset = offset_calculator.get_vertical_offset(cord[1])\n",
    "            cv2.putText(frame,str(round(x_offset))+\" \"+str(round(y_offset)),cord,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0, 255, 0),1,cv2.LINE_AA)\n",
    "\n",
    "    def write_result(self,results,frame):\n",
    "        confidence_threshold = 0.25\n",
    "        \n",
    "        result_to_pandas = results.pandas().xyxy[0]\n",
    "        results = result_to_pandas.to_numpy()\n",
    "       \n",
    "        num_of_objects = len(result_to_pandas)\n",
    "        cords = []\n",
    "        for i in range(num_of_objects):\n",
    "            \n",
    "            if results[i,4] > confidence_threshold:\n",
    "                x_min,y_min,x_max,y_max = int(results[i,0]),int(results[i,1]),int(results[i,2]),int(results[i,3])\n",
    "                cv2.rectangle(frame,(x_min,y_min),(x_max,y_max),(0,255,0),2)\n",
    "\n",
    "                cv2.putText(frame,str(results[i,6]),(x_min,y_min),cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0, 255, 0),1,cv2.LINE_AA)\n",
    "                cords.append((round((x_min+x_max)/2),round((y_min+y_max)/2)))\n",
    "        return cords\n",
    "    \n",
    "    def write_result_depth(self,frame,cords,distances):\n",
    "        for i in range(len(distances)):\n",
    "            cv2.putText(frame,format(distances[i],'.2f'),cords[i],cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,0.5,(0,255,0),1,cv2.LINE_AA)\n",
    "    \n",
    "    def write_depth(self,frame,depth_image,cords):\n",
    "        for cord in cords:\n",
    "            distance = self.detect_depth(depth_image,cord)\n",
    "            cv2.putText(frame,distance,cord,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2,cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "1.23\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(format(1.23423,'.2f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\admin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-7-1 Python-3.10.9 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 1760518 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "object_detect  = ObjectDection(\"vid.mp4\",False) # Since I don't have intel d435i, so I put a false here to use my alternative method to detect distance\n",
    "# Put Ture as a parameter if the camera is intel d435i\n",
    "object_detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n"
     ]
    }
   ],
   "source": [
    "print(\"1\"+\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncap=cv2.VideoCapture(\"vid.mp4\")\\n\\nwhile True:\\n  \\n    ret,frame = cap.read()\\n    hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\\n    lower_blue = np.array([110,50,50])\\n    upper_blue = np.array([130,255,255])\\n    mask =  cv2.inRange(hsv,lower_blue,upper_blue)\\n    contours,hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\\n    print(frame.shape)\\n    roi = []\\n    for contour in contours:\\n        area = cv2.contourArea(contour)\\n        if area >450:\\n            x_min,y_min,width,height = cv2.boundingRect(contour)\\n            ##print(x_min,y_min,width,height)\\n            cv2.rectangle(mask,(x_min-30,y_min-30),(x_min+width+30,y_min+height+30),(2552,255,255),-1)\\n            ##mask[x_min+30:x_min+width-30,y_min+30:y_min+height-30] = 255\\n           ## roi.append([(x_min-30,y_min-30),(x_min+width+30,y_min+height+30)])\\n            cv2.rectangle(frame,(x_min-30,y_min-30),(x_min+width+30,y_min+height+30),(0,255,0),3)\\n\\n    res = cv2.bitwise_and(frame,frame,mask = mask)\\n\\n    cv2.imshow(\"test\",frame)\\n    cv2.imshow(\"hsv\",hsv)\\n    cv2.imshow(\"mask\",mask)\\n    cv2.imshow(\"Filted\",res)\\n    key = cv2.waitKey(2)\\n    if key == 27:\\n        break\\n\\ncap.release()\\ncv2.destroyAllWindows()\\n\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cap=cv2.VideoCapture(\"vid.mp4\")\n",
    "\n",
    "while True:\n",
    "  \n",
    "    ret,frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "    lower_blue = np.array([110,50,50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "    mask =  cv2.inRange(hsv,lower_blue,upper_blue)\n",
    "    contours,hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    print(frame.shape)\n",
    "    roi = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area >450:\n",
    "            x_min,y_min,width,height = cv2.boundingRect(contour)\n",
    "            ##print(x_min,y_min,width,height)\n",
    "            cv2.rectangle(mask,(x_min-30,y_min-30),(x_min+width+30,y_min+height+30),(2552,255,255),-1)\n",
    "            ##mask[x_min+30:x_min+width-30,y_min+30:y_min+height-30] = 255\n",
    "           ## roi.append([(x_min-30,y_min-30),(x_min+width+30,y_min+height+30)])\n",
    "            cv2.rectangle(frame,(x_min-30,y_min-30),(x_min+width+30,y_min+height+30),(0,255,0),3)\n",
    "\n",
    "    res = cv2.bitwise_and(frame,frame,mask = mask)\n",
    "\n",
    "    cv2.imshow(\"test\",frame)\n",
    "    cv2.imshow(\"hsv\",hsv)\n",
    "    cv2.imshow(\"mask\",mask)\n",
    "    cv2.imshow(\"Filted\",res)\n",
    "    key = cv2.waitKey(2)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
