{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObjectDection:\n",
    "    def __init__(self,file1,file2=None,isRealsense=False):\n",
    "        self.video = file1\n",
    "        self.right_camera = file2\n",
    "        self.model = self.load_model()\n",
    "        self.isRealsense = isRealsense\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "        return model\n",
    "    \n",
    "    def camera_setup(self):\n",
    "        pipeline = rs.pipeline()\n",
    "        pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "        config = rs.config()\n",
    "        pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "        device = pipeline_profile.get_device()\n",
    "        device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "        # Start streaming\n",
    "        pipeline.start(config)\n",
    "        return pipeline\n",
    "     \n",
    "\n",
    "    def capture(self):\n",
    "        if self.isRealsense:\n",
    "            pipeline = self.camera_setup()\n",
    "\n",
    "        cap = cv2.VideoCapture(self.video)\n",
    "        #cap_right = cv2.VideoCapture(self.right_camera)\n",
    "        while True:\n",
    "            if self.isRealsense:\n",
    "                ret,color_image,depth_image = self.get_frame(pipeline)\n",
    "            else:\n",
    "                ret,color_image = cap.read()\n",
    "                depth_image = None\n",
    "\n",
    "            if ret:\n",
    "                result_object = self.detect_object(color_image)\n",
    "\n",
    "                cords = self.write_result(result_object,color_image)\n",
    "                if depth_image:\n",
    "                    self.write_depth(color_image,depth_image,cords)\n",
    "                cv2.imshow(\"frame\",color_image)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27 or key == ord(\"q\"):\n",
    "                    break\n",
    "        print(\"Finish\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "\n",
    "    def detect_object(self,color_frame):\n",
    "        #color_frame = [color_frame]\n",
    "        results = self.model(color_frame)\n",
    "        #labels,cord = results.xyxyn[0][:,-1], results.xyxyn[0][:,:-1]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def detect_depth(self,depth_image,point):\n",
    "            return depth_image[point[1], point[0]]\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.capture()\n",
    "\n",
    "    def get_frame(self,pipeline):\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        if not depth_frame or not color_frame:\n",
    "            return False, None, None\n",
    "        return True, depth_image, color_image\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def write_result(self,results,frame):\n",
    "        confidence_threshold = 0.25\n",
    "        \n",
    "        result_to_pandas = results.pandas().xyxy[0]\n",
    "        results = result_to_pandas.to_numpy()\n",
    "       \n",
    "        num_of_objects = len(result_to_pandas)\n",
    "        cords = []\n",
    "        for i in range(num_of_objects):\n",
    "            \n",
    "            if results[i,4] > confidence_threshold:\n",
    "                x_min,y_min,x_max,y_max = int(results[i,0]),int(results[i,1]),int(results[i,2]),int(results[i,3])\n",
    "                cv2.rectangle(frame,(x_min,y_min),(x_max,y_max),(0,255,0),2)\n",
    "\n",
    "                cv2.putText(frame,str(results[i,6]),(x_min,y_min),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),2,cv2.LINE_AA)\n",
    "                cords.append((x_min,y_min))\n",
    "        return cords\n",
    "    \n",
    "    def write_depth(self,frame,depth_image,cords):\n",
    "        for cord in cords:\n",
    "            distance = self.detect_depth(depth_image,cord)\n",
    "            cv2.putText(frame,distance,cord,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2,cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\admin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-7-1 Python-3.10.9 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "object_detect  = ObjectDection(\"video.mp4\")\n",
    "object_detect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
