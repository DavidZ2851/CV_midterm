{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObjectDection:\n",
    "    def __init__(self,left_camera,right_camera,isRealsense=False):\n",
    "        self.left_camera = left_camera\n",
    "        self.right_camera = right_camera\n",
    "        self.model = self.load_model()\n",
    "        self.isRealsense = isRealsense\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "        return model\n",
    "    \n",
    "    def camera_setup(self):\n",
    "        pipeline = rs.pipeline()\n",
    "        pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "        config = rs.config()\n",
    "        pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "        device = pipeline_profile.get_device()\n",
    "        device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "        # Start streaming\n",
    "        pipeline.start(config)\n",
    "        return pipeline\n",
    "     \n",
    "\n",
    "    def capture(self):\n",
    "        if self.isRealsense:\n",
    "            pipeline = self.camera_setup()\n",
    "\n",
    "        cap_left = cv2.VideoCapture(self.left_camera)\n",
    "        cap_left.set(cv2.CAP_PROP_FRAME_HEIGHT,480)\n",
    "        cap_left.set(cv2.CAP_PROP_FRAME_WIDTH,640)\n",
    "        if self.isRealsense== False:\n",
    "            cap_right = cv2.VideoCapture(self.right_camera)\n",
    "        while True:\n",
    "            if self.isRealsense:\n",
    "                ret,color_image,depth_image = self.get_frame(pipeline)\n",
    "            else:\n",
    "                ret,color_image = cap_left.read()\n",
    "                if self.isRealsense== False:\n",
    "                    ret,color_image_2 = cap_right.read()\n",
    "                depth_image = None\n",
    "\n",
    "            if ret:\n",
    "                result_object = self.detect_object(color_image)\n",
    "\n",
    "                cords = self.write_result(result_object,color_image)\n",
    "                if self.isRealsense== False:\n",
    "                    result_object_right = self.detect_object(color_image_2)\n",
    "                    cords_right = self.write_result(result_object_right,color_image_2)\n",
    "                    distances = self.get_depth(cords,cords_right)\n",
    "                    self.write_result_depth(color_image,cords,distances)\n",
    "\n",
    "                if depth_image:\n",
    "                    self.write_depth(color_image,depth_image,cords)\n",
    "                cv2.imshow(\"frame\",color_image)\n",
    "                if self.isRealsense== False:\n",
    "                    cv2.imshow(\"right camera\",color_image_2)\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27 or key == ord(\"q\"):\n",
    "                    break\n",
    "        print(\"Finish\")\n",
    "        cap_left.release()\n",
    "        if self.isRealsense== False:\n",
    "            cap_right.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "\n",
    "    def detect_object(self,color_frame):\n",
    "        #color_frame = [color_frame]\n",
    "        results = self.model(color_frame)\n",
    "        #labels,cord = results.xyxyn[0][:,-1], results.xyxyn[0][:,:-1]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def detect_depth(self,depth_image,point):\n",
    "            return depth_image[point[1], point[0]]\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.capture()\n",
    "\n",
    "    def get_frame(self,pipeline):\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        if not depth_frame or not color_frame:\n",
    "            return False, None, None\n",
    "        return True, depth_image, color_image\n",
    "\n",
    "\n",
    "    def get_depth(self,cords_left,cords_right):\n",
    "        x_center = 640/2 # The camera I used is 480p, 640 pixels * 480 pixels, the center is 640/2\n",
    "        y_center = 480/2\n",
    "        \n",
    "        cam_distance = 13 # distance between two cameras cm\n",
    "        fov = 90 # fov 90 degrees\n",
    "        length = min(len(cords_left),len(cords_right))\n",
    "        \n",
    "        f_pixel = x_center / np.tan(fov * 0.5 * np.pi/180) # find the pixel length cm/\n",
    "        distance = []\n",
    "        for i in range(length):\n",
    "            disparity = cords_left[i][0]-cords_right[i][1] # diparity in pixels\n",
    "            zDepth = f_pixel * cam_distance / disparity\n",
    "            distance.append(abs(zDepth))\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def write_result(self,results,frame):\n",
    "        confidence_threshold = 0.25\n",
    "        \n",
    "        result_to_pandas = results.pandas().xyxy[0]\n",
    "        results = result_to_pandas.to_numpy()\n",
    "       \n",
    "        num_of_objects = len(result_to_pandas)\n",
    "        cords = []\n",
    "        for i in range(num_of_objects):\n",
    "            \n",
    "            if results[i,4] > confidence_threshold:\n",
    "                x_min,y_min,x_max,y_max = int(results[i,0]),int(results[i,1]),int(results[i,2]),int(results[i,3])\n",
    "                cv2.rectangle(frame,(x_min,y_min),(x_max,y_max),(0,255,0),2)\n",
    "\n",
    "                cv2.putText(frame,str(results[i,6]),(x_min,y_min),cv2.FONT_HERSHEY_SIMPLEX,1,(0, 255, 0),2,cv2.LINE_AA)\n",
    "                cords.append((round((x_min+x_max)/2),round((y_min+y_max)/2)))\n",
    "        return cords\n",
    "    \n",
    "    def write_result_depth(self,frame,cords,distances):\n",
    "        for i in range(len(distances)):\n",
    "            cv2.putText(frame,format(distances[i],'.2f'),cords[i],cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,1,(0,255,0),2,cv2.LINE_AA)\n",
    "    \n",
    "    def write_depth(self,frame,depth_image,cords):\n",
    "        for cord in cords:\n",
    "            distance = self.detect_depth(depth_image,cord)\n",
    "            cv2.putText(frame,distance,cord,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2,cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "1.23\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(format(1.23423,'.2f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\admin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-7-1 Python-3.10.9 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "object_detect  = ObjectDection(0,1,False) # Since I don't have intel d435i, so I put a false here to use my alternative method to detect distance\n",
    "# Put Ture as a parameter if the camera is intel d435i\n",
    "object_detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncap=cv2.VideoCapture(0)\\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT,480)\\ncap.set(cv2.CAP_PROP_FRAME_WIDTH,640)\\nwhile True:\\n\\n  \\n    ret,frame = cap.read()\\n    cv2.imshow(\"test\",frame)\\n    key = cv2.waitKey(1)\\n    if key == 27:\\n        break\\n\\ncap.release()\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,480)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,640)\n",
    "while True:\n",
    "\n",
    "  \n",
    "    ret,frame = cap.read()\n",
    "    cv2.imshow(\"test\",frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
